# _____                              _______________________________
# ____/ TELEMAC Project Definitions /______________________________/
#
[Configurations]
#
configs:    serial mpi mpi-submitter py-mpi-submitter py-serial-submitter
#
# _____          ___________________________________________________
# ____/ GENERAL /__________________________________________________/
[general]
#
modules:    system -mascaret
#
cmd_lib:    ar cru <libname> <objs>
#
mods_all:   -I <config>
#
sfx_zip:    .gztar
sfx_lib:    .lib
sfx_obj:    .o
sfx_mod:    .mod
sfx_exe:
#
# _____                                ______________________________
# ____/ SERIAL -- Main EXE, Main node /_____________________________/
[serial]
#
brief: scalar mode on the main node.
   TELEMAC will work whether processor is 0 or 1
#
cmd_obj:    gfortran -c -O3 -cpp -fbounds-check -ffpe-trap=zero,overflow,underflow,invalid -Wall -finit-real=nan -fbacktrace -finit-real=nan -fconvert=big-endian -frecord-marker=4 <mods> <incs> <f95name>
cmd_exe:    gfortran -fconvert=big-endian -frecord-marker=4 -v -lm -lz -o <exename> <objs> <libs>
#
# _____                                 _______________________________
# ____/ MPI -- GERUN/MPIRUN, Main node /______________________________/
[mpi]
#
brief: parallel mode on the compute nodes using gerun/mpirun directly.
   This is used inside a jobscript you submit yourself.
#
mpi_cmdexec: gerun <exename>
# removed -DHAVE_MUMPS after -DHAVE_MPI
cmd_obj:    mpif90 -c -Og -cpp -fbounds-check -ffpe-trap=zero,overflow,underflow,invalid -Wall -finit-real=nan -fbacktrace -finit-real=nan -DHAVE_MPI -fconvert=big-endian -frecord-marker=4 <mods> <incs> <f95name>
cmd_exe:    mpif90 -frecord-marker=4 -fconvert=big-endian -v -lm -lz -o <exename> <objs> <libs>
#
incs_all:  -I$OPENBLASROOT/include -I$METISHOME/include -I$HDF5HOME/include
libs_all:  -L$OPENBLASROOT/lib -lopenblas -L$METISHOME/lib -lmetis -L$HDF5HOME/lib -lhdf5
#
# _____                                           _______________________________
# ____/ MPI-SUBMITTER -- GERUN/MPIRUN, HPC queue /______________________________/
[mpi-submitter]
#
brief: parallel mode with job submission from the login node, using gerun/mpirun.
   In this case, the file partitioning and assembly are done by 
   python on the main node.
   (The merge would have to be done manually)
#
#mpi_hosts:    mgmt01
mpi_cmdexec: gerun <exename>
mpi_hosts:
#
par_cmdexec:   <config>/partel  < PARTEL.PAR >> <partel.log>
#
hpc_stdin: #!/bin/bash -l
   #$ -l h_rt=<walltime>
   #$ -l mem=2G
   #$ -l tmpfs=10G
   #$ -N <jobname>
   #$ -pe mpi <ncsize>
   module unload compilers mpi
   module load compilers/gnu/4.9.2
   module load mpi/openmpi/3.1.4/gnu-4.9.2
   module load metis/5.1.0/gnu-4.9.2
   module load hdf/5-1.8.15/gnu-4.9.2
   module load python2/recommended
   <mpi_cmdexec>
   exit
#
hpc_cmdexec:  chmod 755 <hpc_stdin>; qsub <hpc_stdin>
#
cmd_obj:    mpif90 -c -O3 -cpp -fbounds-check -ffpe-trap=zero,overflow,underflow,invalid -Wall -finit-real=nan -fbacktrace -finit-real=nan -DHAVE_MPI -fconvert=big-endian -frecord-marker=4 <mods> <incs> <f95name>
cmd_exe:    mpif90 -frecord-marker=4 -fconvert=big-endian -lpthread -v -lm -lz -o <exename> <objs> <libs>
#
incs_all:  -I$OPENBLASROOT/include -I$METISHOME/include -I$HDF5HOME/include
libs_all:  -L$OPENBLASROOT/lib -lopenblas -L$METISHOME/lib -lmetis -L$HDF5HOME/lib -lhdf5
#
# _____                                        ________________________________
# ____/ PY-MPI_SUBMITTER -- Python, HPC queue /_______________________________/
[py-mpi-submitter]
#
brief: parallel mode with job submission, using python script within
   the job.
   In this case, the file partitioning and assembly are done by
   python within the job.
#
mpi_hosts:    
mpi_cmdexec: gerun <exename>
#
par_cmdexec:   <config>/partel  < PARTEL.PAR >> <partel.log>
#
hpc_stdin: #!/bin/bash -l
   #$ -l h_rt=<walltime>
   #$ -l mem=2G
   #$ -l tmpfs=10G
   #$ -N <jobname>
   #$ -pe mpi <ncsize>
   module unload compilers mpi
   module load compilers/gnu/4.9.2
   module load mpi/openmpi/3.1.4/gnu-4.9.2
   module load metis/5.1.0/gnu-4.9.2
   module load hdf/5-1.8.15/gnu-4.9.2
   module load python2/recommended
   PATH=$PATH:$HOMETEL/scripts/python27
   export PATH
   cd <wdir>
   <py_runcode>
   exit
#
hpc_runcode:   chmod 755 <hpc_stdin>; qsub <hpc_stdin>
#
hpc_depend: -W depend=afterok:<jobid>
#
cmd_obj:    mpif90 -c -O3 -cpp -fbounds-check -ffpe-trap=zero,overflow,underflow,invalid -Wall -finit-real=nan -fbacktrace -finit-real=nan -DHAVE_MPI -fconvert=big-endian -frecord-marker=4 <mods> <incs> <f95name>
cmd_exe:    mpif90 -frecord-marker=4 -fconvert=big-endian -lpthread -v -lm -lz -o <exename> <objs> <libs>
#
incs_all:  -I$OPENBLASROOT/include -I$METISHOME/include -I$HDF5HOME/include
libs_all:  -L$OPENBLASROOT/lib -lopenblas -L$METISHOME/lib -lmetis -L$HDF5HOME/lib -lhdf5
#
# _____                                           ________________________________
# ____/ PY-SERIAL-SUBMITTER -- Python, HPC queue /_______________________________/
[py-serial-submitter]
#
brief: serial mode on the HPC queue, using python script within
   the job.
   In this case, there is no partitioning or assembly done by
   python within the job.
#
hpc_stdin: #!/bin/bash -l
   #$ -l h_rt=<walltime>
   #$ -l mem=2G
   #$ -l tmpfs=10G
   #$ -N <jobname>
   #$ -pe smp <ncsize>
   module unload compilers mpi
   module load compilers/gnu/4.9.2
   module load mpi/openmpi/3.1.4/gnu-4.9.2
   module load metis/5.1.0/gnu-4.9.2
   module load hdf/5-1.8.15/gnu-4.9.2
   module load python2/recommended
   PATH=$PATH:$HOMETEL/scripts/python27
   export PATH
   cd <wdir>
   <py_runcode>
   exit
#
hpc_depend: -W depend=afterok:<jobid>
#
hpc_runcode:   chmod 755 <hpc_stdin>; qsub <hpc_stdin>
#
cmd_obj:    gfortran -c -O3 -cpp -fbounds-check -ffpe-trap=zero,overflow,underflow,invalid -Wall -finit-real=nan -fbacktrace -finit-real=nan -fconvert=big-endian -frecord-marker=4 <mods> <incs> <f95name>
cmd_exe:    gfortran -fconvert=big-endian -frecord-marker=4 -v -lm -lz -o <exename> <objs> <libs>
#
incs_all:  -I$OPENBLASROOT/include -I$METISHOME/include -I$HDF5HOME/include
libs_all:  -L$OPENBLASROOT/lib -lopenblas -L$METISHOME/lib -lmetis -L$HDF5HOME/lib -lhdf5
#
#
